{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2ndWifiActivityRecognition.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fqbGjRZxKef",
        "colab_type": "text"
      },
      "source": [
        "Note: This code has been inspired from https://github.com/ermongroup/Wifi_Activity_Recognition   and some sections have been copied outright."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xB7_usIqIw4q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Mount Google Drive\n",
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZoJD_hYur62",
        "colab_type": "text"
      },
      "source": [
        "Note place the dataset in the root of Google drive and create these 4 folders in google drive otherwise code may not find some folders and may give error:  \n",
        "1.ActivityRecognition/  \n",
        "2.ActivityRecognition/input_files  \n",
        "3.ActivityRecognition/input_files_2  \n",
        "4.ActivityRecognition/keras_models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xtT8-c9JMi1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Extract \"Dataset\" in WifiActivityRecognition Folder\n",
        "!tar xvzf \"./drive/My Drive/WifiActivityRecognition/Dataset.tar.gz\" -C \"./drive/My Drive/WifiActivityRecognition/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HUWlx1VJZj6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#List the sorted data and make sure data has same naming scheme in both Annotation and Input\n",
        "import glob\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "sorted(glob.glob(\"./drive/My Drive/WifiActivityRecognition/Dataset/annotation_*bed*.csv\"))\n",
        "#sorted(glob.glob(\"./drive/My Drive/WifiActivityRecognition/Dataset/input_*bed*.csv\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1FAizKV0MMAG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Rename siamak and sankalp data with numbers in the start to make sorted input and annotation data consistient\n",
        "#Note this would only work if your folder scheme is the same as mine i.e, WifiActivityRecognition/Dataset\n",
        "#Otherwise understand this block and rename the data yourself\n",
        "import glob\n",
        "import os\n",
        "annot_data = sorted(glob.glob(\"./drive/My Drive/WifiActivityRecognition/Dataset/input_161219*.csv\"))\n",
        "for i in range(len(annot_data)):\n",
        "  print(annot_data[i][1:39+16] + annot_data[i][16+46:])\n",
        "  os.rename(annot_data[i], annot_data[i][0:39+16] + annot_data[i][46+16:])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3pCAdbDSoUY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Convert Raw Data into Windows with threshold of 60%\n",
        "#These files are 1kHz and include noActivity data in each file. We are going to convert these into 500Hz and separate file for each activity after this.\n",
        "#Do it separately for each activity. You just need to change activity name in main and restart runtime after each execution to avoid memory error\n",
        "#Note : You have to mount GDrive each time you restart runtume and clear the ram\n",
        "#Colab takes about ~10GB ram for a 4GB csv file and max ram availabl is 12GB. This is why we need to process each activity separately on colab.\n",
        "\n",
        "import numpy as np,numpy\n",
        "import csv\n",
        "import glob\n",
        "import os\n",
        "\n",
        "window_size = 1000\n",
        "threshold = 60\n",
        "slide_size = 200 #less than window_size!!!\n",
        "\n",
        "def dataimport(path1, path2):\n",
        "\n",
        "\txx = np.empty([0,window_size,90],float)\n",
        "\tyy = np.empty([0,8],float)\n",
        "\n",
        "\t###Input data###\n",
        "\t#data import from csv\n",
        "\tinput_csv_files = sorted(glob.glob(path1))\n",
        "\tfor f in input_csv_files:\n",
        "\t\tprint(\"input_file_name=\",f)\n",
        "\t\tdata = [[ float(elm) for elm in v] for v in csv.reader(open(f, \"r\"))]\n",
        "\t\ttmp1 = np.array(data)\n",
        "\t\tx2 =np.empty([0,window_size,90],float)\n",
        "\n",
        "\t\t#data import by slide window\n",
        "\t\tk = 0\n",
        "\t\twhile k <= (len(tmp1) + 1 - 2 * window_size):\n",
        "\t\t\tx = np.dstack(np.array(tmp1[k:k+window_size, 1:91]).T)\n",
        "\t\t\tx2 = np.concatenate((x2, x),axis=0)\n",
        "\t\t\tk += slide_size\n",
        "\n",
        "\t\txx = np.concatenate((xx,x2),axis=0)\n",
        "\txx = xx.reshape(len(xx),-1)\n",
        "\n",
        "\t###Annotation data###\n",
        "\t#data import from csv\n",
        "\tannotation_csv_files = sorted(glob.glob(path2))\n",
        "\tfor ff in annotation_csv_files:\n",
        "\t\tprint(\"annotation_file_name=\",ff)\n",
        "\t\tano_data = [[ str(elm) for elm in v] for v in csv.reader(open(ff,\"r\"))]\n",
        "\t\ttmp2 = np.array(ano_data)\n",
        "\n",
        "\t\t#data import by slide window\n",
        "\t\ty = np.zeros(((len(tmp2) + 1 - 2 * window_size)//slide_size+1,8))\n",
        "\t\tk = 0\n",
        "\t\twhile k <= (len(tmp2) + 1 - 2 * window_size):\n",
        "\t\t\ty_pre = np.stack(np.array(tmp2[k:k+window_size]))\n",
        "\t\t\tbed = 0\n",
        "\t\t\tfall = 0\n",
        "\t\t\twalk = 0\n",
        "\t\t\tpickup = 0\n",
        "\t\t\trun = 0\n",
        "\t\t\tsitdown = 0\n",
        "\t\t\tstandup = 0\n",
        "\t\t\tnoactivity = 0\n",
        "\t\t\tfor j in range(window_size):\n",
        "\t\t\t\tif y_pre[j] == \"bed\":\n",
        "\t\t\t\t\tbed += 1\n",
        "\t\t\t\telif y_pre[j] == \"fall\":\n",
        "\t\t\t\t\tfall += 1\n",
        "\t\t\t\telif y_pre[j] == \"walk\":\n",
        "\t\t\t\t\twalk += 1\n",
        "\t\t\t\telif y_pre[j] == \"pickup\":\n",
        "\t\t\t\t\tpickup += 1\n",
        "\t\t\t\telif y_pre[j] == \"run\":\n",
        "\t\t\t\t\trun += 1\n",
        "\t\t\t\telif y_pre[j] == \"sitdown\":\n",
        "\t\t\t\t\tsitdown += 1\n",
        "\t\t\t\telif y_pre[j] == \"standup\":\n",
        "\t\t\t\t\tstandup += 1\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\tnoactivity += 1\n",
        "\n",
        "\t\t\tif bed > window_size * threshold / 100:\n",
        "\t\t\t\ty[int(k/slide_size),:] = np.array([0,1,0,0,0,0,0,0])\n",
        "\t\t\telif fall > window_size * threshold / 100:\n",
        "\t\t\t\ty[int(k/slide_size),:] = np.array([0,0,1,0,0,0,0,0])\n",
        "\t\t\telif walk > window_size * threshold / 100:\n",
        "\t\t\t\ty[int(k/slide_size),:] = np.array([0,0,0,1,0,0,0,0])\n",
        "\t\t\telif pickup > window_size * threshold / 100:\n",
        "\t\t\t\ty[int(k/slide_size),:] = np.array([0,0,0,0,1,0,0,0])\n",
        "\t\t\telif run > window_size * threshold / 100:\n",
        "\t\t\t\ty[int(k/slide_size),:] = np.array([0,0,0,0,0,1,0,0])\n",
        "\t\t\telif sitdown > window_size * threshold / 100:\n",
        "\t\t\t\ty[int(k/slide_size),:] = np.array([0,0,0,0,0,0,1,0])\n",
        "\t\t\telif standup > window_size * threshold / 100:\n",
        "\t\t\t\ty[int(k/slide_size),:] = np.array([0,0,0,0,0,0,0,1])\n",
        "\t\t\telse:\n",
        "\t\t\t\ty[int(k/slide_size),:] = np.array([1,0,0,0,0,0,0,0])\n",
        "\t\t\tk += slide_size\n",
        "\n",
        "\t\tyy = np.concatenate((yy, y),axis=0)\n",
        "\tprint(xx.shape,yy.shape)\n",
        "\treturn (xx, yy)\n",
        "\n",
        "\n",
        "#### Main ####\n",
        "if not os.path.exists(\"input_files/\"):\n",
        "        os.makedirs(\"input_files/\")\n",
        "\n",
        "for i, label in enumerate ([\"bed\"]):\n",
        "\tfilepath1 = \"./drive/My Drive/WifiActivityRecognition/Dataset/input_*\" + str(label) + \"*.csv\"\n",
        "\tfilepath2 = \"./drive/My Drive/WifiActivityRecognition/Dataset/annotation_*\" + str(label) + \"*.csv\"\n",
        "\toutputfilename1 = \"./drive/My Drive/WifiActivityRecognition/input_files/xx_\" + str(window_size) + \"_\" + str(threshold) + \"_\" + label + \".csv\"\n",
        "\toutputfilename2 = \"./drive/My Drive/WifiActivityRecognition/input_files/yy_\" + str(window_size) + \"_\" + str(threshold) + \"_\" + label + \".csv\"\n",
        "\n",
        "\tx, y = dataimport(filepath1, filepath2)\n",
        "\twith open(outputfilename1, \"w\") as f:\n",
        "\t\twriter = csv.writer(f, lineterminator=\"\\n\")\n",
        "\t\twriter.writerows(x)\n",
        "\twith open(outputfilename2, \"w\") as f:\n",
        "\t\twriter = csv.writer(f, lineterminator=\"\\n\")\n",
        "\t\twriter.writerows(y)\n",
        "\tprint(label + \"finish!\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wT_sB3c-tI4d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Number of windows/samples_size of a particular activity in a csv file\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import csv\n",
        "\n",
        "file = pd.read_csv(\"./drive/My Drive/ActivityRecognition/input_files/yy_1000_60_bed.csv\")\n",
        "file = np.array(file)\n",
        "np.shape(np.where(file[:,1] == 1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQF2Ny3qtULR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Now Keeping Only activity Data and Discarding NoActivity Data and converting to 500Hz to limit RAM usage and avoid memory error.\n",
        "#Again you need to repeat this with every activity file.\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import csv\n",
        "import glob\n",
        "\n",
        "\n",
        "def csv_write():\n",
        "    window_size = 500\n",
        "    threshold = 60\n",
        "    print(\"csv file importing...\")\n",
        "\n",
        "    for i in [\"walk\"]:\n",
        "        label = i\n",
        "        SKIPROW = 2 #Skip every 2 rows -> overlap 800ms to 600ms  (To avoid memory error)\n",
        "        num_lines = sum(1 for l in open(\"./drive/My Drive/input_files/xx_1000_60_\" + str(i) + \".csv\"))\n",
        "        skip_idx = [x for x in range(1, num_lines) if x % SKIPROW !=0]\n",
        "\n",
        "        xx = np.array(pd.read_csv(\"./drive/My Drive/WifiActivityRecognition/input_files/xx_1000_60_\" + str(i) + \".csv\", header=None, skiprows = skip_idx))\n",
        "        yy = np.array(pd.read_csv(\"./drive/My Drive/WifiActivityRecognition/input_files/yy_1000_60_\" + str(i) + \".csv\", header=None, skiprows = skip_idx))\n",
        "        print(\"Read Done\")\n",
        "        # eliminate the NoActivity Data\n",
        "        rows = np.where(yy[:,0] == 1)\n",
        "        xx = np.delete(xx, rows,0)\n",
        "        print(\"Eliminate Done Done\")\n",
        "\n",
        "        xx = xx.reshape(len(xx),1000,90)\n",
        "        \n",
        "        # 1000 Hz to 500 Hz (To avoid memory error)\n",
        "        xx = xx[:,::2,:90]\n",
        "\n",
        "        #Rehsape Back to save in CSV\n",
        "        xx = xx.reshape(-1,500*90)\n",
        "        print(\"Downsampling Done\")\n",
        "        print(str(i), \"finished...\", \"xx=\", xx.shape, \"yy=\",  yy.shape)\n",
        "        \n",
        "        #Write Data in CSV Files to be imported to train LSTM\n",
        "        outputfilename1 = \"./drive/My Drive/WifiActivityRecognition/input_files_2/xxx_\" + str(window_size) + \"_\" + str(threshold) + \"_\" + label + \".csv\"\n",
        "        with open(outputfilename1, \"w\") as f:\n",
        "          writer = csv.writer(f, lineterminator=\"\\n\")\n",
        "          writer.writerows(xx)\n",
        "        print(label + \"finish!\")\n",
        "       \n",
        "      \n",
        "csv_write()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRFVsnlHzkUz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Importing Data for LSTM\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import glob\n",
        "import csv\n",
        "x = np.empty(shape = [0,500,90], dtype = float)\n",
        "y = np.empty(shape = [0,7], dtype = float)\n",
        "for i in [\"bed\", \"fall\", \"pickup\", \"run\", \"sitdown\", \"standup\", \"walk\"]:\n",
        "  f = \"./drive/My Drive/WifiActivityRecognition/input_files_2/xxx_500_60_\" + str(i) + \".csv\"\n",
        "  print(\"input_file_name=\",f)\n",
        "  data = [[ float(elm) for elm in v] for v in csv.reader(open(f, \"r\"))]\n",
        "  tmp1 = np.array(data)\n",
        "  tmp1 = np.reshape(tmp1,[-1,500,90])\n",
        "  r,c,w = np.shape(tmp1)\n",
        "  x = np.concatenate((x,tmp1),axis = 0)\n",
        "  yy = np.empty([r,7],float)\n",
        "  if i == \"bed\" :\n",
        "     yy[:,:] = np.array([1.0,0.0,0.0,0.0,0.0,0.0,0.0])\n",
        "  elif i == \"fall\":\n",
        "     yy[:,:] = np.array([0.0,1.0,0.0,0.0,0.0,0.0,0.0])\n",
        "  elif i == \"pickup\":\n",
        "     yy[:,:] = np.array([0.0,0.0,1.0,0.0,0.0,0.0,0.0])\n",
        "  elif i == \"run\":\n",
        "     yy[:,:] = np.array([0.0,0.0,0.0,1.0,0.0,0.0,0.0])\n",
        "  elif i == \"sitdown\":\n",
        "     yy[:,:] = np.array([0.0,0.0,0.0,0.0,1.0,0.0,0.0])\n",
        "  elif i == \"standup\":\n",
        "     yy[:,:] = np.array([0.0,0.0,0.0,0.0,0.0,1.0,0.0])\n",
        "  elif i == \"walk\":\n",
        "     yy[:,:] = np.array([0.0,0.0,0.0,0.0,0.0,0.0,1.0])\n",
        "      \n",
        "  y = np.concatenate((y,yy),axis = 0)\n",
        "  \n",
        "  print(np.shape(x))\n",
        "  print(np.shape(y))\n",
        "  print(str(i) + \"done\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLal89ICsaE9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "perm = np.arange(x.shape[0])\n",
        "np.random.shuffle(perm)\n",
        "x = x[perm]\n",
        "y = y[perm]\n",
        "\n",
        "div_idx = int(np.ceil(0.9 * x.shape[0]))\n",
        "x_train, x_test = x[:div_idx, ...], x[div_idx:, ...]\n",
        "y_train, y_test = y[:div_idx, ...], y[div_idx:, ...]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxAhfZeAykU7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Save perm from drive (In case if training stops and you want to recover test and training split from drive)\n",
        "\n",
        "outputfilename1 = \"./drive/My Drive/WifiActivityRecognition/perm/perm.csv\"\n",
        "with open(outputfilename1, \"w\") as f:\n",
        "  writer = csv.writer(f, lineterminator=\"\\n\")\n",
        "  writer.writerows(map(lambda n: [n], perm))\n",
        "\t\t"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APoIPSQb4_v4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Load perm from drive\n",
        "f = \"./drive/My Drive/WifiActivityRecognition/perm/perm2.csv\"\n",
        "print(\"input_file_name=\",f)\n",
        "perm = np.array([[ int(elm) for elm in v] for v in csv.reader(open(f, \"r\"))])\n",
        "perm = perm[:]\n",
        "x = x[perm]\n",
        "y = y[perm]\n",
        "\n",
        "train_idx = int(np.ceil(0.8 * x.shape[0]))\n",
        "val_idx = int(np.ceil(0.9 * x.shape[0]))\n",
        "\n",
        "x_train, x_val, x_test = x[:train_idx, ...], x[train_idx:val_idx:, ...], x[val_idx:, ...]\n",
        "y_train, y_val, y_test = y[:train_idx, ...], y[train_idx:val_idx:, ...], y[val_idx:, ...]\n",
        "\n",
        "x_train, x_val, x_test = np.squeeze(x_train), np.squeeze(x_val), np.squeeze(x_test)\n",
        "y_train, y_val, y_test = np.squeeze(y_train), np.squeeze(y_val), np.squeeze(y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOxi1XNSz9g8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Machine Learning Model\n",
        "#Plot Loss\n",
        "%matplotlib inline\n",
        "\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Flatten, Dense, Activation\n",
        "\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "\n",
        "from tensorflow.python.keras.models import Sequential\n",
        "from tensorflow.python.keras.layers import Dense, GRU, Embedding, LSTM,Dropout\n",
        "from tensorflow.python.keras.optimizers import Adam,SGD,RMSprop\n",
        "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.python.keras.layers import BatchNormalization\n",
        "from keras import metrics\n",
        "\n",
        "\n",
        "class PlotLosses(keras.callbacks.Callback):\n",
        "    def on_train_begin(self, logs={}):\n",
        "        self.i = 0\n",
        "        self.x = []\n",
        "        self.losses = []\n",
        "        self.val_losses = []\n",
        "        \n",
        "        self.fig = plt.figure()\n",
        "        \n",
        "        self.logs = []\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        \n",
        "        self.logs.append(logs)\n",
        "        self.x.append(self.i)\n",
        "        self.losses.append(logs.get('loss'))\n",
        "        self.val_losses.append(logs.get('val_loss'))\n",
        "        self.i += 1\n",
        "        \n",
        "        clear_output(wait=True)\n",
        "        plt.plot(self.x, self.losses, label=\"loss\")\n",
        "        plt.plot(self.x, self.val_losses, label=\"val_loss\")\n",
        "        plt.legend()\n",
        "        plt.show();\n",
        "        \n",
        "plot_losses = PlotLosses()\n",
        "\n",
        "\n",
        "class PlotLearning(keras.callbacks.Callback):\n",
        "    def on_train_begin(self, logs={}):\n",
        "        self.i = 0\n",
        "        self.x = []\n",
        "        self.losses = []\n",
        "        self.val_losses = []\n",
        "        self.acc = []\n",
        "        self.val_acc = []\n",
        "        self.fig = plt.figure()\n",
        "        \n",
        "        self.logs = []\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        \n",
        "        self.logs.append(logs)\n",
        "        self.x.append(self.i)\n",
        "        self.losses.append(logs.get('loss'))\n",
        "        self.val_losses.append(logs.get('val_loss'))\n",
        "        self.acc.append(logs.get('acc'))\n",
        "        self.val_acc.append(logs.get('val_acc'))\n",
        "        self.i += 1\n",
        "        f, (ax1, ax2) = plt.subplots(1, 2, sharex=True)\n",
        "        \n",
        "        clear_output(wait=True)\n",
        "        \n",
        "        ax1.set_yscale('log')\n",
        "        ax1.plot(self.x, self.losses, label=\"loss\")\n",
        "        ax1.plot(self.x, self.val_losses, label=\"val_loss\")\n",
        "        ax1.legend()\n",
        "        \n",
        "        ax2.plot(self.x, self.acc, label=\"accuracy\")\n",
        "        ax2.plot(self.x, self.val_acc, label=\"validation accuracy\")\n",
        "        ax2.legend()\n",
        "        \n",
        "        plt.show();\n",
        "        \n",
        "plot = PlotLearning()\n",
        "\n",
        "#Machine Learning Model for TPU\n",
        "%matplotlib inline\n",
        "print(keras.__version__)\n",
        "model = Sequential()\n",
        "model.add(LSTM(200, input_shape=(500,90),unit_forget_bias=True,bias_initializer=\"zeros\",return_sequences = False))\n",
        "model.add(Dense(7,activation = 'softmax'))\n",
        "optimizer=tf.train.AdamOptimizer(learning_rate=0.0001)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=optimizer,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "TPU_WORKER = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "tf.logging.set_verbosity(tf.logging.INFO)\n",
        "\n",
        "tpu_model = tf.contrib.tpu.keras_to_tpu_model(\n",
        "    model,\n",
        "    strategy=tf.contrib.tpu.TPUDistributionStrategy(\n",
        "        tf.contrib.cluster_resolver.TPUClusterResolver(TPU_WORKER)))\n",
        "\n",
        "tpu_model.summary()\n",
        "tpu_model.save('./drive/My Drive/WifiActivityRecognition/keras_models/7-activity-2/model_arch.json', overwrite=True)\n",
        "\n",
        "mc = keras.callbacks.ModelCheckpoint('./drive/My Drive/WifiActivityRecognition/keras_models/7-activity-2/weights{epoch:08d}.h5', \n",
        "                                     save_weights_only=True, period=50)\n",
        "\n",
        "history = tpu_model.fit(x_train, y_train,\n",
        "                        epochs=2000,\n",
        "                        batch_size=128 * 8,\n",
        "                        validation_split=0.2,shuffle = True,callbacks=[plot,mc])\n",
        "\n",
        "tpu_model.save('./drive/My Drive/WifiActivityRecognition/keras_models/7-activity-2/model.h5', overwrite=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUKdtr-62ZGM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Make confustion Matrix\n",
        "#Machine Learning Predictor for TPU\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import glob\n",
        "import os\n",
        "\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "\n",
        "from tensorflow.python.keras.models import Sequential\n",
        "from tensorflow.python.keras.layers import Dense, GRU, Embedding, LSTM,Dropout,TimeDistributed, Bidirectional, Dropout\n",
        "from tensorflow.python.keras.optimizers import Adam,SGD,RMSprop\n",
        "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.python.keras.layers import BatchNormalization\n",
        "from mlxtend.evaluate import confusion_matrix\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Bidirectional(LSTM(200), input_shape=(500, 90), merge_mode = 'concat'))\n",
        "model.add(Dropout(0.6))\n",
        "model.add(Dense(8, activation='softmax'))\n",
        "\n",
        "optimizer=tf.train.AdamOptimizer(learning_rate=0.0001)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=optimizer,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.load_weights('./drive/My Drive/WifiActivityRecognition/keras_models/8-activity-3/model00001950.h5')\n",
        "\n",
        "n=np.shape(x_test)\n",
        "n= n[0] - n[0]%8;\n",
        "x_test = x_test[:n,:,:];\n",
        "y_test = y_test[:n,:];\n",
        "pred = model.predict(x_test)\n",
        "\n",
        "cm = confusion_matrix(y_target=np.argmax(y_test,1),y_predicted=np.argmax(pred,1),binary=False)\n",
        "cm\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}